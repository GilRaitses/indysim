\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{fontspec}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{longtable}

\setmainfont{Charter}
\setsansfont{Helvetica Neue}

\titleformat{\section}{\Large\bfseries}{}{0em}{}
\titleformat{\subsection}{\large\bfseries}{}{0em}{}

\title{Speaker Notes\\Sensorimotor Habituation in \textit{Drosophila} Larvae}
\author{Gil Raitses}
\date{Syracuse University \textbullet{} December 2025}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Slide 1: Title}

\textbf{Opening}

Thank you for having me. I will present work on sensorimotor habituation in \textit{Drosophila} larvae, covering both our population-level modeling success and our subsequent attempt to extend the approach to individual phenotyping.

\textbf{Transition}

The presentation has two parts. The first covers the original study where I developed a temporal kernel model. The second covers the follow-up study where I tested whether the same model could phenotype individual larvae.

\section{Slide 2: Original Study Overview}

\textbf{Key Points to Emphasize}

The gamma-difference kernel has two timescales that govern behavior. Fast excitation at $\tau_1 \approx 0.3$ seconds captures the initial sensory response. Slow suppression at $\tau_2 \approx 4$ seconds produces habituation across repeated stimuli. The model was validated across 14 experiments and 701 unique tracks.

\textbf{Audience Anchor}

If there is one thing to remember from the original study, it is that larval reorientation dynamics can be captured by a simple parametric model with biologically interpretable timescales.

\section{Slide 3: Kernel Structure}

\textbf{Figure Walkthrough}

Left panel shows the combined kernel representing the full temporal response. Right panel shows the decomposition into fast gamma in green and slow gamma in red. The fast component peaks at 0.3 seconds and drives immediate response. The slow component peaks around 4 seconds and produces delayed suppression.

\textbf{Mathematical Point}

The kernel $K(t)$ modulates reorientation hazard rate. Positive values increase turning probability. Negative values suppress it. The crossover from positive to negative creates the characteristic excitation-then-inhibition pattern.

\textbf{Connection to Biology}

These timescales may correspond to distinct neural circuit mechanisms. The fast component could reflect direct sensory activation. The slow component could reflect adaptation or inhibitory feedback.

\section{Slide 4: PSTH Computation Methods}

\textbf{Two Approaches}

Empirical PSTH uses direct histogram binning of event times relative to LED onset. No functional form is assumed. Bins are 100ms wide.

Parametric PSTH is derived from the Bernoulli event model. At each timestep, event probability $p(t) = 1 - \exp(-\lambda(t) \cdot \Delta t)$.

\textbf{Key Distinction}

Empirical PSTH is what is observed. Parametric PSTH is what the model predicts. Model fitting minimizes the gap between them.

\textbf{Why This Matters}

Understanding both approaches is essential for interpreting model validation and simulation accuracy.

\section{Slide 5: Simulation Track Generation}

\textbf{Bernoulli Point Process}

This is how simulated tracks are generated.

Step 1: Compute hazard rate at each timestep as $\lambda(t) = \lambda_0 \exp(K(t) + \eta)$.

Step 2: Convert to probability as $p(t) = 1 - \exp(-\lambda(t) \cdot \Delta t)$.

Step 3: Draw Bernoulli sample at each timestep.

Step 4: Enforce 2-second refractory period.

\textbf{Key Parameters}

Baseline intercept controls mean event rate. Track-level standard deviation $\eta$ controls across-track variance.

\textbf{Connection to Phenotyping}

Simulations with known ground truth allow testing whether clustering can recover true phenotypes.

\section{Slide 6: Parameter Sweep for Simulation Calibration}

\textbf{The Optimization Problem}

Simulations must match empirical data. Two parameters control the event distribution: intercept controls mean event rate, and track standard deviation controls variance across tracks.

\textbf{Grid Search}

Swept intercept from $-7.0$ to $-6.0$ and standard deviation from $0.1$ to $0.8$. Compared each combination to empirical distribution using KS test.

\textbf{Optimal Values Found}

Intercept $= -6.54$ and track standard deviation $= 0.38$ produce 14.9 events per 10-minute track with realistic variance.

\section{Slide 7: Simulated vs Empirical Event Counts}

\textbf{Validation Message}

Before using the model for anything, it must be confirmed that it generates realistic data. Panel A shows the histograms overlap well. Panel B shows the box plots match.

\textbf{Key Numbers}

260 empirical tracks and 300 simulated tracks both show median around 15 events per track. Simulated tracks used optimized intercept $= -6.54$ and standard deviation $= 0.38$.

\textbf{Why This Matters}

The simulation framework is the foundation for power analysis. If simulations do not match empirical data, power calculations are meaningless.

\section{Slide 8: Habituation Dynamics}

\textbf{Behavioral Phenomenon}

Turn fraction increases across LED pulses in all four experimental conditions. Larvae spend more time turning and less time running as the session progresses.

\textbf{Condition Comparison}

The 0-250 Cycling condition shows the strongest habituation effect with slope $+0.031$ per pulse. The 50-250 conditions show weaker effects. Shaded bands are 95\% confidence intervals.

\textbf{Interpretation}

Habituation is the behavioral manifestation of the slow suppressive component accumulating across pulses. The kernel model predicts this effect.

\section{Slide 9: Behavioral State Analysis}

\textbf{Detailed State Breakdown}

Gray represents forward running. Pink represents turning. Blue represents pausing. Orange represents reverse crawling.

\textbf{Key Observation}

Turning fraction increases dramatically. Pausing remains below 5\% throughout. Habituation manifests as increased turning, not increased pausing or freezing.

\textbf{Quantitative Point}

By pulse 17 in the 50-250 Cycling condition, larvae spend nearly 40\% of their time turning compared to about 20\% at pulse 0.

\section{Slide 10: Leave-One-Experiment-Out Validation}

\textbf{What This Shows}

Leave-one-experiment-out cross-validation tests whether kernel parameters estimated from 13 experiments generalize to the held-out experiment.

\textbf{Key Result}

Pass rate of 50\% falls within the null distribution with $p = 0.618$. Cross-experiment generalization is no better than chance.

\textbf{Interpretation}

The population model fits well overall, but individual experiments show high variability. This foreshadows the individual-level problems addressed in the follow-up study.

\textbf{Transition}

This result motivated the follow-up question: Can individual larvae be phenotyped using their unique kernel parameters?

\section{Slide 11: Follow-Up Study Overview}

\textbf{Key Points to Emphasize}

The answer to individual phenotyping is negative with current protocols. Sparse data with only 18 to 25 events per track makes 6-parameter estimation unreliable. Apparent clusters are statistical artifacts of fitting high-dimensional models to low-event tracks. Only 8.6\% of tracks show genuine individual differences.

\textbf{Audience Anchor}

The follow-up study is a negative result. Individual phenotyping could not be achieved. But the negative result is informative because it identifies the root cause and points toward solutions.

\section{Slide 12: The Clustering Illusion}

\textbf{Figure Walkthrough}

Panel A shows PCA reveals unimodal distribution, not discrete clusters. Panel B shows all four validation methods fail with ARI below 0.13. Panel C shows gap statistic is minimized at $k=1$, indicating no clusters.

\textbf{Key Message}

K-means will always produce $k$ clusters regardless of whether true clusters exist. The gap statistic tells us $k=1$ is optimal. There are no discrete phenotypes in this data.

\textbf{Why It Matters}

Clusters identified by unsupervised learning are artifacts of sparse data, not genuine biological phenotypes. Publishing these clusters would be misleading.

\section{Slide 13: Data Sparsity Explains Instability}

\textbf{The Math Problem}

Mean 25 events per track. Six kernel parameters to estimate. Data-to-parameter ratio is 4:1. Reliable MLE requires at least 10:1.

\textbf{Visual Explanation}

Panel C shows the calculation: 4 parameters divided by 25 events equals a ratio of 6:1. This is fundamentally underdetermined.

\textbf{Key Number}

100 events per track is the target for stable estimation. Current protocols deliver only 25.

\section{Slide 14: Hierarchical Shrinkage}

\textbf{What Shrinkage Does}

Bayesian hierarchical estimation pulls individual estimates toward the population mean. Tracks with sparse data shrink more. Tracks with abundant data retain their individual estimates.

\textbf{Key Insight}

Shrinkage is not a bug. It is optimal regularization under the assumption that individuals are exchangeable members of a population.

\textbf{Limitation}

Shrinkage cannot create information that is absent. With only 25 events, almost all individual estimates shrink heavily toward the population mean.

\section{Slide 15: The Identifiability Problem}

\textbf{Figure Walkthrough}

Panel A shows continuous design produces high bias and RMSE. Panel B shows burst design extracts 10 times more Fisher Information per event. Panel C shows MLE recovery differs dramatically by design. Panel D shows continuous fails because inhibition dominates during LED-ON.

\textbf{Key Insight}

The problem is not just data quantity but data quality. Continuous 10-second LED pulses produce events during the suppressive phase of the kernel. These events carry almost no information about $\tau_1$.

\textbf{Recommendation Preview}

Switch to burst stimulation to sample the early excitatory window repeatedly.

\section{Slide 16: Stimulation Protocol Comparison}

\textbf{Four Designs Shown}

Panel A shows current continuous 10s ON, 20s OFF. Panel B shows recommended burst 10 pulses of 0.5s with 2s spacing. Panel C shows alternative 4 pulses of 1s with 5s spacing. Panel D shows alternative 2 pulses of 2s with 10s spacing.

\textbf{Key Numbers}

Burst design provides 8 times more Fisher Information than continuous. This could reduce the number of events required for reliable estimation from 100 to 30.

\section{Slide 17: Kernel Model Comparison}

\textbf{Why Compare Models}

The gamma-difference kernel was chosen for interpretability, but verification is needed that it fits as well as flexible alternatives.

\textbf{Results}

Raised cosine basis achieves $R^2 = 0.974$ with 12 parameters. Gamma-difference achieves $R^2 = 0.968$ with 6 parameters.

\textbf{Interpretation}

The gamma-difference captures 96.8\% of the variance explained by the flexible model with half the parameters. The timescales $\tau_1$ and $\tau_2$ are not just curve-fitting artifacts. They represent genuine temporal structure.

\section{Slide 18: Protocol Modification}

\textbf{Primary Recommendation}

Replace continuous 10-second ON periods with burst trains. Each burst event carries 10 times more Fisher Information.

\textbf{Quantitative Benefit}

This modification alone could reduce the number of events required for reliable estimation from 100 to approximately 30.

\textbf{Implementation}

Change the LED control code to deliver 10 pulses of 0.5 seconds each with 2-second spacing instead of a single 10-second pulse.

\section{Slide 19: Extended Recording}

\textbf{Secondary Recommendation}

Target 40 minutes or more of recording to achieve at least 50 reorientation events per track.

\textbf{Current State}

Current 10 to 20 minute recordings yield only 18 to 25 events.

\textbf{Power Analysis Result}

100 events are required for 80\% power to detect a 0.2-second difference in $\tau_1$ at the individual level.

\section{Slide 20: Model Simplification}

\textbf{Approach}

Reduce the parameter space by fixing population-derived parameters.

\textbf{Specific Suggestion}

Fix $\tau_2$ at the population estimate of 3.8 seconds. Fix the amplitude ratio $B/A$ at the population value. Estimate only the fast timescale $\tau_1$ per individual track.

\textbf{Rationale}

Hierarchical Bayesian estimation provides natural regularization toward the population mean. With only one free parameter, even 25 events may be sufficient.

\section{Slide 21: Alternative Phenotypes}

\textbf{Pragmatic Alternative}

Use robust composite phenotypes that avoid kernel fitting entirely.

\textbf{Examples}

ON/OFF event ratio measures whether larvae respond preferentially during LED-ON versus LED-OFF. First-event latency measures time from LED onset to first reorientation.

\textbf{Advantage}

These phenotypes require only event counts, not full 6-parameter kernel estimation.

\section{Slide 22: Within-Condition Analysis}

\textbf{Methodological Point}

Analyze individual differences within experimental conditions rather than pooling across conditions.

\textbf{Why This Matters}

When data from different stimulation intensities and temporal patterns are pooled, condition effects dominate and mask genuine individual variation.

\textbf{Evidence}

The ARI near zero across all validation methods indicates no reproducible structure when pooling.

\section{Slide 23: Original Study Summary}

\textbf{Summary of Success}

The gamma-difference kernel accurately models population-level dynamics. Two timescales govern behavior: $\tau_1 \approx 0.3$s for excitation and $\tau_2 \approx 4$s for suppression. The model is robust across experimental conditions. Biological interpretability comes with equivalent goodness of fit.

\section{Slide 24: Follow-Up Study Summary}

\textbf{Summary of Challenge}

Individual phenotyping fails with current protocols due to sparse data. Apparent clusters are statistical artifacts. Only 8.6\% of tracks show individual variation exceeding noise. Current protocols achieve only 20 to 30\% statistical power.

\textbf{Bottom Line}

Population-level analysis is robust and biologically meaningful. Individual phenotyping requires experimental redesign before kernel-based classification becomes reliable.

\section{Slide 25: Thank You}

\textbf{Transition to Questions}

I am happy to take questions. For common questions, I have prepared some FAQ slides.

\section{Slides 26 to 30: FAQ}

\textbf{Original Study Methods Sequence}

Data collection, then MAGAT trajectory extraction, then event detection, then population kernel fitting, then LOEO validation.

\textbf{Follow-Up Study Methods Sequence}

Individual MLE fitting, then K-means and hierarchical clustering, then round-trip validation, then power analysis, then identifiability analysis.

\textbf{Why Population Succeeds but Individual Fails}

Data-to-parameter ratio. Population pools approximately 15,000 events for 6 parameters, a ratio of 2500 to 1. Individual uses approximately 25 events for 6 parameters, a ratio of 4 to 1.

\textbf{What is Hierarchical Shrinkage}

Bayesian regularization that pulls individual estimates toward the population mean proportionally to data sparsity.

\textbf{How to Interpret Clustering Results}

With extreme skepticism. K-means will always produce $k$ clusters. The gap statistic shows $k=1$ is optimal. Round-trip validation shows ARI below 0.2.

\section{Timing Guide}

\begin{longtable}{lll}
\toprule
Slides & Section & Target Time \\
\midrule
1 to 2 & Introduction & 2 min \\
3 to 7 & PSTH and Simulation Methods & 5 min \\
8 to 10 & Original Study Results & 6 min \\
11 to 17 & Follow-Up Study & 10 min \\
18 to 22 & Recommendations & 5 min \\
23 to 25 & Summary & 3 min \\
26 to 30 & FAQ if needed & 5 min \\
\bottomrule
\end{longtable}

\textbf{Total: 31 to 36 minutes}

\section{Technical Terms}

\textbf{Gamma-difference kernel} Difference of two gamma distributions, one fast for excitation and one slow for suppression.

\textbf{PSTH} Peri-stimulus time histogram, the empirical distribution of event times relative to stimulus onset.

\textbf{Fisher Information} Measure of how much information an observable contains about an unknown parameter.

\textbf{Hierarchical shrinkage} Bayesian regularization toward population mean.

\textbf{Gap statistic} Method for determining optimal number of clusters by comparing within-cluster dispersion to null reference.

\textbf{ARI} Adjusted Rand Index, measure of agreement between two clusterings corrected for chance.

\textbf{MLE} Maximum likelihood estimation.

\textbf{LOEO} Leave-one-experiment-out cross-validation.

\section{Slide 31: Data Structures and Extraction Methods}

\textbf{MAGAT Analyzer}

Marc Gershow developed MAGAT Analyzer for extracting larval trajectories from video recordings. The software performs behavioral state segmentation, identifying when larvae are running, turning, or performing head swings. Reorientation events are detected as state transitions from RUN to TURN. These discrete event times are the primary input for hazard modeling.

\textbf{Run Tables}

Mason Klein developed the run table methodology for organizing behavioral data. Each row represents one run segment between two reorientations. Key fields include run duration in seconds, run distance in millimeters, mean speed during the run, total heading change, and LED state at run onset. Run tables enable analysis of how behavioral parameters change across experimental conditions.

\textbf{Events Group}

The events group is a complementary data structure that records each reorientation onset as a discrete event. Fields include timestamp, position coordinates, LED state, and summary statistics from the preceding run. The events group is used for kernel fitting because it directly counts reorientation events without the run-level aggregation.

\textbf{Why Both Exist}

Run tables and events groups serve different analytical purposes. Run tables characterize the structure of forward movement periods. Events groups characterize the timing of reorientation decisions. Kernel fitting requires event times, so the events group is the primary input. Run tables provide quality filtering by identifying tracks with successful MAGAT segmentation.

\section{Slide 32: From Counting to Simulation}

\textbf{Connecting to Traditional Methods}

Most behavioral analysis uses counting and visualization. Event counts per condition. Heatmaps of spatial position. Histograms of event timing. These methods describe what happened but cannot predict new outcomes.

\textbf{The Extension}

The kernel-based hazard model is generative. Given stimulation timing and kernel parameters, it produces event probabilities at every moment. The model does not just summarize past data. It predicts future behavior under conditions that have not been tested.

\textbf{Practical Value for Experiment Design}

Fisher Information analysis revealed that burst stimulation extracts 10 times more information per event than continuous. This insight emerged from simulation, not from running additional experiments. Power analysis determined that 100 events are needed for individual phenotyping. Protocol designers can use this threshold before collecting any new data.

\textbf{Round-Trip Validation}

Simulation allows testing of analysis pipelines with known ground truth. Clustering algorithms can be validated by generating data from known phenotypes and checking whether the algorithm recovers them. This is impossible with empirical data where ground truth is unknown.

\textbf{Key Message}

Simulation modeling transforms descriptive behavioral science into predictive science. It bridges the gap between what was observed and what will happen, enabling rational experiment design and rigorous validation.

\section{Slides 33-35: MagatFairy}

\textbf{What It Does}

MagatFairy converts MAGAT Analyzer experiments from MATLAB format to clean H5 files. The tool bundles essential MAGAT core classes and uses the MATLAB Engine for Python to perform batch conversion. A single command can process entire genotype directories.

\textbf{Why It Matters}

MATLAB files are difficult to work with in Python. H5 provides a standardized, portable format that enables consistent downstream analysis. MagatFairy ensures that track positions, velocities, behavioral states, and LED timing are extracted correctly.

\textbf{In This Project}

All 14 experiments were converted using MagatFairy. The consolidated dataset containing 701 tracks was assembled from the H5 outputs. The same pipeline can process new experiments to ensure format consistency.

\textbf{Repository}

github.com/GilRaitses/magatfairy

\section{Slides 36-38: RetroVibez}

\textbf{What It Does}

RetroVibez detects reverse crawling behavior in larval trajectories. The detection uses SpeedRunVel, computed as the dot product of heading and velocity vectors. Negative values sustained for at least 3 seconds indicate reversal events.

\textbf{The Four-Stage Pipeline}

Stage 1 runs MATLAB analysis headlessly to compute SpeedRunVel and detect reversals. Stage 2 generates figures in parallel using Python. Stage 3 builds a Quarto document with embedded figures. Stage 4 renders to PDF and HTML.

\textbf{Attribution}

The reversal detection algorithm implements methods from Klein et al. 2015. The core MATLAB script is at matlab/mason\_analysis.m. RetroVibez packages this into an automated, reproducible pipeline.

\textbf{Going Forward}

New experiments can be processed with the same pipeline. Reports are generated automatically with consistent formatting. The modular design allows adding new behavioral metrics.

\textbf{Repository}

github.com/GilRaitses/retrovibez

\section{Supplemental Slides}

The following supplemental slides provide additional detail on population-level analyses, model validation, and design optimization.

\subsection{S1. Population-Level Data Summary}

\textbf{Key Statistics}

The dataset contains 14 experiments with 701 larval tracks across four stimulation conditions. Only 19\% of tracks meet all quality thresholds for kernel fitting. Track completeness varies by experiment, with some experiments having higher drop-out rates due to tracking failures.

\textbf{Condition Effects}

Kernel timescales vary 3.9-fold across conditions. The intensity manipulation from 0 to 250 versus 50 to 250 PWM produces the largest effect size at Hedges g of 2.4. Background cycling has a modest effect with g of 0.6.

\textbf{Interpretation}

The population-level model performs well across all conditions except 50-250 Cycling, which shows reduced fit quality. Condition effects are interpretable through the factorial design.

\subsection{S2. Early vs Late Habituation}

\textbf{Key Finding}

Turn fraction increases substantially from early to late pulses across all conditions. The 0-250 Constant condition shows the strongest effect, rising from 44\% to 77\%.

\textbf{Interpretation}

Progressive habituation develops throughout the stimulation protocol. Larvae become increasingly likely to turn as they experience more light pulses. The model captures this effect through the suppression timescale tau2.

\subsection{S3. PSTH to Kernel Fitting}

\textbf{Panel Description}

Panel A shows the empirical PSTH computed by histogram binning. Panel B shows the fitted kernel with suppression trough at 2.5 seconds. Panel C converts kernel values to Bernoulli event probabilities.

\textbf{Model Logic}

The kernel modulates the baseline hazard rate. Positive values increase event probability while negative values suppress it. The fitted kernel captures the empirical pattern of initial excitation followed by sustained suppression.

\subsection{S4. Posterior Predictive Checks}

\textbf{PPC Metrics}

Event count compares the number of simulated events to observed. Mean ISI compares the average interval between events. PSTH shape uses the Kolmogorov-Smirnov statistic to compare temporal distributions.

\textbf{Results}

Event count and mean ISI show modest pass rates at 53.6\%. PSTH shape fails almost universally at 4.4\%. Only 37.2\% of tracks pass all three checks.

\textbf{Interpretation}

The model captures overall event rates but not the detailed temporal structure of individual responses. Individual heterogeneity exceeds what the population model can capture.

\subsection{S5. Simulated Larval Trajectories}

\textbf{Visualization}

Upper panels show XY movement paths with turn locations marked in red. Lower panels show event timing relative to LED-ON periods in yellow.

\textbf{Event Variability}

The three example tracks show 1.2 to 2.8 events per minute, illustrating natural variation in reorientation rates. The simulation generates realistic heterogeneity.

\subsection{S6. Factorial Design Analysis}

\textbf{Design}

The 2x2 factorial manipulates intensity step and background pattern. Intensity step compares 0 to 250 versus 50 to 250 PWM. Background pattern compares constant at 7 PWM versus cycling between 5 and 15 PWM.

\textbf{Results}

Intensity modulation produces a significant negative effect on alpha, indicating faster suppression onset. The cycling-by-intensity interaction is significant. Rebound effect gamma shows the largest uncertainty.

\subsection{S7. Validation Journey}

\textbf{Narrative}

The figure traces the progression from apparent phenotypes to continuous variation. Initial clustering suggests four distinct phenotypes with high classification accuracy. Round-trip validation reveals that clusters are artifacts. Only 8.6\% of tracks show genuine variation exceeding measurement noise.

\textbf{Key Message}

K-means will always find clusters. The critical step is validation, which requires round-trip testing with known ground truth.

\subsection{S8. Design Comparison Summary}

\textbf{Current Protocol}

Continuous 10-second ON and 20-second OFF produces only 1.9 events per track per LED cycle. RMSE for tau1 estimation is 0.108 seconds.

\textbf{Burst Protocol}

Ten 0.5-second pulses increase event yield 8-fold to 14.9 per track. RMSE drops to 0.036 seconds, a 3-fold improvement.

\textbf{Recommendation}

Switching to burst stimulation would substantially improve individual phenotyping feasibility without extending recording duration. The tradeoff between event yield and information per event favors burst design.

\end{document}

